% -*- coding: utf-8 -*-
@inproceedings{Sarikaya2011Deep,
  title={Deep belief nets for natural language call-routing},
  author={Sarikaya, Ruhi and Hinton, Geoffrey E. and Ramabhadran, Bhuvana},
  booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages={5680-5683},
  year={2011},
 keywords={RBM;Call-Routing;Deep Learning;DBN},
}

@article{SlocumMachine,
  title={Machine Translation at Texas: The Early Years},
  author={Slocum, Jonathan and Lehmann, Winfred P.},
  journal={University of Texas at Austin Linguistics Research Center},
}

@article{Pardelli2008From,
  title={From Weaver to the ALPAC Report},
  author={Pardelli, Gabriella and Sassi, Manuela and Goggi, Sara},
  year={2008},
}

@article{Earley1970An,
  title={An efficient context-free parsing algorithm},
  author={Earley, Jay},
  journal={Comm Acm},
  volume={26},
  number={1},
  pages={57-61},
  year={1970},
 keywords={compilers;computational complexity;context-free grammar;parsing;syntax analysis},
 abstract={Translation from Commun. ACM 13, 94‒102 (1970; Zbl 0185.43401).},
}

@article{Chen2011Maximum,
  title={Maximum Entropy Principle for Uncertain Variables},
  author={Chen and Xiaowei and Dai and Wei},
  journal={International Journal of Fuzzy Systems},
  volume={13},
  number={3},
  pages={232-236},
  year={2011},
 keywords={Uncertain variable;entropy;maximum entropy principle},
 abstract={Abstract The concept of uncertain entropy is used to provide a quantitative measurement of the uncertainty associated with uncertain variables. After introducing the definition, this paper gives some examples of entropy of uncertain variables. Furthermore this paper proposes the maximum entropy principle for uncertain variables, that is, out of all the uncertainty distributions satisfying given constraints, to choose the one has maximum entropy.},
}

@article{Ravuri2015Recurrent,
  title={Recurrent Neural Network and LSTM Models for Lexical Utterance Classification},
  author={Ravuri, Suman and Stolcke, Andreas},
  year={2015},
}

@article{Mesnil2013Investigation,
  title={Investigation of recurrent-neural-network architectures and learning methods for spoken language understanding},
  author={Mesnil, G. and He, X. and Deng, L. and Bengio, Y.},
  journal={Interspeech},
  year={2013},
}

@book{Graves2012Long,
  title={Long Short-Term Memory},
  author={Graves, Alex},
  publisher={Springer Berlin Heidelberg},
  pages={1735-1780},
  year={2012},
 keywords={Long short-term memory, Artículo},
 abstract={As discussed in the previous chapter, an important benefit of recurrent neural networks is their ability to use contextual information when mapping between input and output sequences. Unfortunately, for standard RNN architectures, the range of context that can be in practice accessed is quite limited. The problem is that the influence of a given input on the hidden layer, and therefore on the network output, either decays or blows up exponentially as it cycles around the network鈥檚 recurrent connections. This effect is often referred to in the literature as the vanishing gradient problem  (Hochreiter, 1991; Hochreiter et al., 2001a; Bengio et al., 1994). The vanishing gradient problem is illustrated schematically in Figure 4.1},
}

@article{Tai2015Improved,
  title={Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks},
  author={Tai, Kai Sheng and Socher, Richard and Manning, Christopher D.},
  journal={Computer Science},
  volume={5},
  number={1},
  pages={: 36.},
  year={2015},
 keywords={Computer Science - Computation and Language;Computer Science - Artificial Intelligence;Computer Science - Learning},
 abstract={Abstract:  Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).},
}

@article{Bahdanau2014Neural,
  title={Neural Machine Translation by Jointly Learning to Align and Translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={Computer Science},
  year={2014},
 keywords={Computer Science - Computation and Language;Computer Science - Learning;Computer Science - Neural and Evolutionary Computing;Statistics - Machine Learning},
 abstract={Abstract:  Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
}

@article{Rumelhart1986Learning,
  title={Learning internal representations by error propagation: Parallel Distributed Processing, Volume 1: Foundations},
  author={Rumelhart, D. E. and Hinton, G. E. and Williams, R. J.},
  year={1986},
}

@inproceedings{Turian2010Word,
  title={Word representations: a simple and general method for semi-supervised learning},
  author={Turian, Joseph and Ratinov, Lev and Bengio, Yoshua},
  booktitle={ACL 2010, Proceedings of the  Meeting of the Association for Computational Linguistics, July 11-16, 2010, Uppsala, Sweden},
  pages={384-394},
  year={2010},
 abstract={ABSTRACT  If we take an existing supervised NLP sys- tem, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accu- racy of these baselines. We find further improvements by combining di erent word representations. You can download our word features, for o -the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize. com/projects/wordreprs/},
}

@article{Mikolov2013Distributed,
  title={Distributed Representations of Words and Phrases and their Compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  volume={26},
  pages={3111-3119},
  year={2013},
 keywords={Computer Science - Computation and Language;Computer Science - Learning;Statistics - Machine Learning},
 abstract={Abstract:  The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
}

@inproceedings{Pennington2014Glove,
  title={Glove: Global Vectors for Word Representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  pages={1532-1543},
  year={2014},
 abstract={Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. The model pro- duces a vector space with meaningful sub- structure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on simi- larity tasks and named entity recognition.},
}

@article{Hays1960Grouping,
  title={Grouping and dependency theories.},
  author={Hays, David G},
  journal={In H. P. Edmundson (Ed.), Proceedings of the National Symposium on Machine Translation. Englewood Cliffs, N. J.: Prentice-Hall},
  year={1960},
 keywords={GROUPS(MATHEMATICS;TOPOLOGY;TOPOLOGY;GROUPS(MATHEMATICS;LANGUAGE;TOPOLOGY;MACHINE TRANSLATION;VOCABULARY;SET THEORY},
 abstract={Immediate-constituent analysis and dependency analysis (two theories of syntactic description) are based, respectively, on the topologies of grouping and of trees. A correspondence between structures of the two types is defined, and the two topologies are compared, mainly in terms of their empirical applications. (Author)},
}

@article{The,
  title={The Stanford NLP (Natural Language Processing) Group},
}

@article{Kingma2014Adam,
  title={Adam: A Method for Stochastic Optimization},
  author={Kingma, Diederik and Ba, Jimmy},
  journal={Computer Science},
  year={2014},
 keywords={Computer Science - Learning},
 abstract={Abstract:  We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
}

@inproceedings{Pieraccini1992A,
  title={A speech understanding system based on statistical representation of semantics},
  author={Pieraccini, R. and Tzoukermann, E. and Gorelov, Z. and Gauvain, J. L.},
  booktitle={IEEE International Conference on Acoustics, Speech, and Signal Processing},
  pages={193-196 vol.1},
  year={1992},
 keywords={SQL;natural languages;relational databases;speech recognition;statistical analysis;DARPA test;English sentences;SQL code;SQL translator;acoustic signal},
 abstract={An understanding system, designed for both speech and text input, has been implemented based on statistical representation of task specific semantic knowledge. The core of the system is the conceptual decoder, which extracts the words and their association to the conceptual structure of the task directly from the acoustic signal. The conceptual information, which is also used to clarify the English sentences, is encoded following a statistical paradigm. A template generator and an SQL (structured query language) translator process the sentence and produce SQL code for querying a relational database. Results of the system on the official DARPA test are given},
}

@article{Wang2005Spoken,
  title={Spoken language understanding},
  author={Wang, Ye Yi and Deng, Li and Acero, A},
  journal={Signal Processing Magazine IEEE},
  volume={22},
  number={5},
  pages={16-31},
  year={2005},
 keywords={hidden Markov models;speech recognition;statistical analysis;hidden Markov models;human language technology;intelligent human-machine communication;multiple-word block;segment models;speech recognition;speech technology},
 abstract={This article is intended to serve as an introduction to the field of statistical SLU, based on the mainstream statistical modeling approach that shares a similar mathematical framework with many other statistical pattern recognition applications such as speech recognition. In particular, we formulated a number of statistical models for SLU in the literature as extensions to HMMs as segment models, where a multiple-word block (segment) with word dependency is generated from each underlying Markov state corresponding to each individual semantic slot defined from the application domain. In the past, due partly to its nature of symbolic rather than numeric processing, the important field of SLU in human language technology has not been widely exposed to the signal processing research community. However, many key techniques in SLU originated from statistical signal processing. And because SLU is becoming increasingly important, as one major target application area of ASR that has been dear to many signal processing researchers, we contribute this article to provide a natural bridge between ASR and SLU in methodological and mathematical foundation. It is our hope that when the mathematical basis of SLU becomes well known through this introductory article, more powerful techniques established by signal processing researchers may further advance SLU to form a solid application area, making speech technology a successful component for intelligent human-machine communication.},
}

@article{Tur2016Towards,
  title={Towards Deeper Understanding Deep Convex Networks for Semantic Utterance Classification},
  author={Tur, Gokhan and Deng, Li and Hakkanitür, Dilek and He, Xiaodong},
  volume={22},
  number={10},
  pages={5045-5048},
  year={2016},
 keywords={acoustic signal processing;feature extraction;interactive systems;learning (artificial intelligence;pattern classification;speech recognition;speech-based user interfaces;DCN-based method;SUC accuracy;acoustic modeling},
 abstract={Following the recent advances in deep learning techniques, in this paper, we present the application of special type of deep architecture — deep convex networks (DCNs) — for semantic utterance classification (SUC). DCNs are shown to have several advantages over deep belief networks (DBNs) including classification accuracy and training scalability. However, adoption of DCNs for SUC comes with non-trivial issues. Specifically, SUC has an extremely sparse input feature space encompassing a very large number of lexical and semantic features. This is about a few thousand times larger than the feature space for acoustic modeling, yet with a much smaller number of training samples. Experimental results we obtained on a domain classification task for spoken language understanding demonstrate the effectiveness of DCNs. The DCN-based method produces higher SUC accuracy than the Boosting-based discriminative classifier with word trigrams.},
}

@book{Tur2011Spoken,
  title={Spoken Language Understanding: Systems for Extracting Semantic Information from Speech},
  author={Tur, Gokhan and Mori, Renato De},
  year={2011},
 abstract={ABSTRACT  This chapter reviews the evolution of methods for spoken language understanding (SLU) systems. It provides Meaning Representation Language (MRL) with methods for obtaining meaning representations from natural language. The chapter introduces probabilistic frameworks accounting for knowledge imprecision and limitations of automatic speech recognition systems. It reviews automatic systems for SLU using these methods. Computer epistemology deals with the representation of semantic knowledge in a computer using an appropriate formalism. Furthermore, especially for SLU, signs used for interpretation are extracted from the speech signal with a process that is not perfect. These problems suggested the use of probabilistic models and machine learning methods for automatically characterizing supports for semantic constituents end their relations. As a consequence, methods were proposed for estimating the parameters of generative models and classification methods. The chapter reviews these methods, and reports some evaluations. natural language processing; speech recognition equipment},
}

@inproceedings{Haffner2003Optimizing,
  title={Optimizing SVMs for complex call classification},
  author={Haffner, P. and Tur, G. and Wright, J. H.},
  booktitle={IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings},
  pages={I-632-I-635 vol.1},
  year={2003},
 keywords={interactive systems;learning automata;natural language interfaces;optimisation;pattern classification;speech recognition;AT&amp;T How May I Help You dialog system;Adaboost;HMIHY natural dialog system},
 abstract={) natural dialog system by 50 %.},
}

@inproceedings{Chelba2003Speech,
  title={Speech utterance classification},
  author={Chelba, C. and Mahajan, M. and Acero, A.},
  booktitle={IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings},
  pages={I-280-I-283 vol.1},
  year={2003},
 keywords={Bayes methods;grammars;maximum entropy methods;maximum likelihood estimation;signal classification;speech recognition;ATIS corpus;Naive Bayes classifier;classification accuracy;concurrent speech classification},
 abstract={The paper presents a series of experiments on speech utterance classification performed on the ATIS corpus. We compare the performance of n-gram classifiers with that of Naive Bayes and maximum entropy classifiers. The n-gram classifiers have the advantage that one can use a single pass system (concurrent speech recognition and classification) whereas for Naive Bayes or maximum entropy classification we use a two-stage system: speech recognition followed by classification. Substantial relative improvements (up to 55%) in classification accuracy can be obtained using discriminative. training methods that belong to the class of conditional maximum likelihood techniques.},
}

@inproceedings{Chen2015Deriving,
  title={Deriving local relational surface forms from dependency-based entity embeddings for unsupervised spoken language understanding},
  author={Chen, Yun Nung and Hakkani-Tür, Dilek and Tur, Gokan},
  booktitle={Spoken Language Technology Workshop},
  pages={242-247},
  year={2015},
 keywords={spoken dialogue systems (SDS;spoken language understanding (SLU;relation detection;semantic knowledge graph;entity embeddings},
 abstract={Recent works showed the trend of leveraging web-scaled structured semantic knowledge resources such as Freebase for open domain spoken language understanding (SLU). Knowledge graphs provide sufficient but ambiguous relations for the same entity, which can be used as statistical background knowledge to infer possible relations for interpretation of user utterances. This paper proposes an approach to capture the relational surface forms by mapping dependency-based contexts of entities from the text domain to the spoken domain. Relational surface forms are learned from dependency-based entity embeddings, which encode the contexts of entities from dependency trees in a deep learning model. The derived surface forms carry functional dependency to the entities and convey the explicit expression of relations. The experiments demonstrate the efficiency of leveraging derived relational surface forms as local cues together with prior background knowledge.},
}

@inproceedings{Xu2014Convolutional,
  title={Convolutional neural network based triangular CRF for joint intent detection and slot filling},
  author={Xu, Puyang and Sarikaya, Ruhi},
  booktitle={Automatic Speech Recognition and Understanding},
  pages={78-83},
  year={2014},
 keywords={triangular CRF;Joint modeling;slot filling;convolutional neural network},
}

@article{Deoras2013Deep,
  title={Deep belief network based semantic taggers for spoken language understanding},
  author={Deoras, A. and Sarikaya, R.},
  journal={Inproceedings},
  year={2013},
}

@inproceedings{Liu2014Query,
  title={Query understanding enhanced by hierarchical parsing structures},
  author={Liu, Jingjing and Pasupat, Panupong and Wang, Yining and Cyphers, Scott and Glass, Jim},
  booktitle={Automatic Speech Recognition and Understanding},
  pages={72-77},
  year={2014},
 keywords={linguistic parsing;query understanding;semantic tagging},
 abstract={Query understanding has been well studied in the areas of information retrieval and spoken language understanding (SLU). There are generally three layers of query understanding: domain classification, user intent detection, and semantic tagging. Classifiers can be applied to domain and intent detection in real systems, and semantic tagging (or slot filling) is commonly defined as a sequence-labeling task - mapping a sequence of words to a sequence of labels. Various statistical features (e.g., n-grams) can be extracted from annotated queries for learning label prediction models; however, linguistic characteristics of queries, such as hierarchical structures and semantic relationships, are usually neglected in the feature extraction process. In this work, we propose an approach that leverages linguistic knowledge encoded in hierarchical parse trees for query understanding. Specifically, for natural language queries, we extract a set of syntactic structural features and semantic dependency features from query parse trees to enhance inference model learning. Experiments on real natural language queries show that augmenting sequence labeling models with linguistic knowledge can improve query understanding performance in various domains.},
}

@inproceedings{Chen2015Matrix,
  title={Matrix Factorization with Knowledge Graph Propagation for Unsupervised Spoken Language Understanding},
  author={Chen, Yun Nung and Wang, William Yang and Gershman, Anatole and Rudnicky, Alexander I},
  booktitle={The  Meeting of the Association for Computational Linguistics and the  International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing},
  year={2015},
 abstract={Spoken dialogue systems (SDS) typically require a predefined semantic ontology to train a spoken language understanding (SLU) module. In addition to the annotation cost, a key challenge for designing such an ontology is to define a coherent slot set while considering their complex relations. This paper introduces a novel matrix factorization (MF) approach to learn latent feature vectors for utterances and semantic elements without the need of corpus annotations. Specifically, our model learns the semantic slots for a domain-specific SDS in an unsupervised fashion, and carries out semantic parsing using latent MF techniques. To further consider the global semantic structure, such as inter-word and inter-slot relations, we augment the latent MF-based model with a knowledge graph propagation model based on a slot-based semantic graph and a word-based lexical graph. Our experiments show that the proposed MF approaches produce better SLU models that are able to predict semantic slots and word patterns taking into account their relations and domain-specificity in a joint manner.},
}

